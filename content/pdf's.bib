
@article{hachaj_application_2015,
	title = {Application of Assistive Computer Vision Methods to Oyama Karate Techniques Recognition},
	volume = {7},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/7/4/1670},
	doi = {10.3390/sym7041670},
	abstract = {In this paper we propose a novel algorithm that enables online actions segmentation and classification. The algorithm enables segmentation from an incoming motion capture ({MoCap}) data stream, sport (or karate) movement sequences that are later processed by classification algorithm. The segmentation is based on Gesture Description Language classifier that is trained with an unsupervised learning algorithm. The classification is performed by continuous density forward-only hidden Markov models ({HMM}) classifier. Our methodology was evaluated on a unique dataset consisting of {MoCap} recordings of six Oyama karate martial artists including multiple champion of Kumite Knockdown Oyama karate. The dataset consists of 10 classes of actions and included dynamic actions of stands, kicks and blocking techniques. Total number of samples was 1236. We have examined several {HMM} classifiers with various number of hidden states and also Gaussian mixture model ({GMM}) classifier to empirically find the best setup of the proposed method in our dataset. We have used leave-one-out cross validation. The recognition rate of our methodology differs between karate techniques and is in the range of 81\% ± 15\% even to 100\%. Our method is not limited for this class of actions but can be easily adapted to any other {MoCap}-based actions. The description of our approach and its evaluation are the main contributions of this paper. The results presented in this paper are effects of pioneering research on online karate action classification.},
	pages = {1670--1698},
	number = {4},
	journaltitle = {Symmetry},
	author = {Hachaj, Tomasz and Ogiela, Marek R. and Koptyra, Katarzyna},
	urldate = {2024-08-20},
	date = {2015-12},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {actions segmentation, assistive computer vision, Gestures Description Language, hidden Markov models, Oyama karate, sport actions recognition, unsupervised learning},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/WN7HVJAK/Hachaj et al. - 2015 - Application of Assistive Computer Vision Methods to Oyama Karate Techniques Recognition.pdf:application/pdf},
}

@article{echeverria_kumitron_2021,
	title = {{KUMITRON}: Artificial Intelligence System to Monitor Karate Fights that Synchronize Aerial Images with Physiological and Inertial Signals},
	url = {https://dl.acm.org/doi/10.1145/3397482.3450730},
	doi = {10.1145/3397482.3450730},
	shorttitle = {{KUMITRON}},
	abstract = {New technologies make it possible to develop tools that allow more efficient and personalized interaction in unsuspected areas such as martial arts. From the point of view of the modelling of human movement in relation to the learning of complex motor skills, martial arts are of interest because they are articulated around a system of movements that are predefined -or at least, bounded- and governed by the Laws of Physics. Their execution must be learned after continuous practice over time. Artificial Intelligence algorithms can be used to obtain motion patterns that can be used to compare a learners’ practice against the execution of an expert, as well as to analyse its temporal evolution during learning. In this paper we introduce {KUMITRON}, which collects motion data from wearable sensors and integrates computer vision and machine learning algorithms to help karate practitioners improve their skills in combat. The current version focuses on using the computer vision algorithms to identify the anticipation of the opponent's movements. This information is computed in real time and can be communicated to the learner together with a recommendation of the type of strategy to use in the combat.},
	pages = {37--39},
	journaltitle = {26th International Conference on Intelligent User Interfaces},
	author = {Echeverria, Jon and C. Santos, Olga},
	urldate = {2024-08-20},
	date = {2021-04-14},
	langid = {english},
	note = {Conference Name: {IUI} '21: 26th International Conference on Intelligent User Interfaces
{ISBN}: 9781450380188
Place: College Station {TX} {USA}
Publisher: {ACM}},
}

@article{al-faris_review_2020,
	title = {A Review on Computer Vision-Based Methods for Human Action Recognition},
	volume = {6},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/6/6/46},
	doi = {10.3390/jimaging6060046},
	abstract = {Human action recognition targets recognising different actions from a sequence of observations and different environmental conditions. A wide different applications is applicable to vision based action recognition research. This can include video surveillance, tracking, health care, and human–computer interaction. However, accurate and effective vision based recognition systems continue to be a big challenging area of research in the field of computer vision. This review introduces the most recent human action recognition systems and provides the advances of state-of-the-art methods. To this end, the direction of this research is sorted out from hand-crafted representation based methods including holistic and local representation methods with various sources of data, to a deep learning technology including discriminative and generative models and multi-modality based methods. Next, the most common datasets of human action recognition are presented. This review introduces several analyses, comparisons and recommendations that help to find out the direction of future research.},
	pages = {46},
	number = {6},
	journaltitle = {Journal of Imaging},
	author = {Al-Faris, Mahmoud and Chiverton, John and Ndzi, David and Ahmed, Ahmed Isam},
	urldate = {2024-08-29},
	date = {2020-06},
	langid = {english},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, feature representation, hand-crafted feature, human action recognition},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/7HKMVT7D/Al-Faris et al. - 2020 - A Review on Computer Vision-Based Methods for Human Action Recognition.pdf:application/pdf},
}

@article{wang_deep_2019,
	title = {Deep learning for sensor-based activity recognition: A survey},
	volume = {119},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S016786551830045X},
	doi = {10.1016/j.patrec.2018.02.010},
	series = {Deep Learning for Pattern Recognition},
	shorttitle = {Deep learning for sensor-based activity recognition},
	abstract = {Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. Since then, deep learning based methods have been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent advance of deep learning based sensor-based activity recognition. We summarize existing literature from three aspects: sensor modality, deep model, and application. We also present detailed insights on existing work and propose grand challenges for future research.},
	pages = {3--11},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Wang, Jindong and Chen, Yiqiang and Hao, Shuji and Peng, Xiaohui and Hu, Lisha},
	urldate = {2024-08-30},
	date = {2019-03-01},
	keywords = {Activity recognition, Deep learning, Pattern recognition, Pervasive computing},
	file = {ScienceDirect Snapshot:/Users/jjmr/Zotero/storage/327Q7TCH/S016786551830045X.html:text/html;Versión enviada:/Users/jjmr/Zotero/storage/AQ3U6W6L/Wang et al. - 2019 - Deep learning for sensor-based activity recognition A survey.pdf:application/pdf},
}

@article{azmat_aerial_2023,
	title = {Aerial Insights: Deep Learning-Based Human Action Recognition in Drone Imagery},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10210047},
	doi = {10.1109/ACCESS.2023.3302353},
	shorttitle = {Aerial Insights},
	abstract = {Human action recognition is critical because it allows machines to comprehend and interpret human behavior, which has several real-world applications such as video surveillance, robot-human collaboration, sports analysis, and entertainment. The enormous variety in human motion and appearance is one of the most challenging problems in human action recognition. Additionally, when drones are employed for video capture, the complexity of recognition gets enhanced manyfold. The challenges including the dynamic background, motion blur, occlusions, video capture angle, and exposure issues gets introduced that need to be taken care of. In this article, we proposed a system that deal with the mentioned challenges in drone recorded red-green-blue ({RGB}) videos. The system first splits the video into its constituent frames and then performs a focused smoothing operation on the frames utilizing a bilateral filter. As a result, the foreground objects in the image gets enhanced while the background gets blur. After that, a segmentation operation is performed using a quick shift segmentation algorithm that separates out human silhouette from the original video frame. The human skeleton was extracted from the silhouette, and key-points on the skeleton were identified. Thirteen skeleton key-points were extracted, including the head, left wrist, right wrist, left elbow, right elbow, torso, abdomen, right thigh, left thigh, right knee, left knee, right ankle, and left ankle. Using these key-points, we extracted normalized positions, their angular and distance relationship with each other, and 3D point clouds. By implementing an expectation maximization algorithm based on the Gaussian mixture model, we drew elliptical clusters over the pixels using the key-points as the central positions to represent the human silhouette. Landmarks were located on the boundaries of these ellipses and were tracked from the beginning until the end of activity. After optimizing the feature matrix using a naïve Bayes feature optimizer, the classification is performed using a deep convolutional neural network. For our experimentation and the validation of our system, three benchmark datasets were utilized i.e., the {UAVGesture}, the {DroneAction}, and the {UAVHuman} dataset. Our model achieved a respective action recognition accuracy of 0.95, 0.90, and 0.44 on the mentioned datasets.},
	pages = {83946--83961},
	journaltitle = {{IEEE} Access},
	author = {Azmat, Usman and Alotaibi, Saud S. and Abdelhaq, Maha and Alsufyani, Nawal and Shorfuzzaman, Mohammad and Jalal, Ahmad and Park, Jeongmin},
	urldate = {2024-08-30},
	date = {2023},
	note = {Conference Name: {IEEE} Access},
	keywords = {Behavioral sciences, Cameras, Convolutional neural network, Convolutional neural networks, Drones, expectation maximization, Feature extraction, Gaussian mixture model, Human activity recognition, Image segmentation, quadratic discriminant analysis, Quadrature phase shift keying, quick-shift segmentation, Three-dimensional displays, video processing},
	file = {IEEE Xplore Abstract Record:/Users/jjmr/Zotero/storage/MXV8VULD/10210047.html:text/html;IEEE Xplore Full Text PDF:/Users/jjmr/Zotero/storage/C73E7L3S/Azmat et al. - 2023 - Aerial Insights Deep Learning-Based Human Action Recognition in Drone Imagery.pdf:application/pdf},
}

@article{bhatt_cnn_2021,
	title = {{CNN} Variants for Computer Vision: History, Architecture, Application, Challenges and Future Scope},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/20/2470},
	doi = {10.3390/electronics10202470},
	shorttitle = {{CNN} Variants for Computer Vision},
	abstract = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep {CNN} (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for {CNN} study. Several inspirational concepts for the progress of {CNN} have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep {CNN}. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep {CNN} architectures, and it divides numerous recent developments in {CNN} architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based {CNN} are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in {CNN} by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the {CNN}’s components, the strengths and weaknesses of various {CNN} variants, research gap or open challenges, {CNN} applications, and the future research direction.},
	pages = {2470},
	number = {20},
	journaltitle = {Electronics},
	author = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
	urldate = {2024-08-30},
	date = {2021-01},
	langid = {english},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {attention-based {CNN}, {CNN}, computer vision, deep {CNN}, feature-map exploitation, object recognition},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/JUIEEKNF/Bhatt et al. - 2021 - CNN Variants for Computer Vision History, Architecture, Application, Challenges and Future Scope.pdf:application/pdf},
}

@article{yang_cnn-lstm_2020,
	title = {{CNN}-{LSTM} deep learning architecture for computer vision-based modal frequency detection},
	volume = {144},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327020302715},
	doi = {10.1016/j.ymssp.2020.106885},
	abstract = {The conventional modal analysis involves physically-attached wired or wireless sensors for vibration measurement of structures. However, this method has certain disadvantages, owing to the sensor’s weight and its low spatial resolution, which limits the analysis precision or the high cost of optical vibration sensors. Besides, the sensor installation and calibration in itself is a time consuming and labor-intensive process. Non-contact computer vision-based vibration measurement techniques can address the shortcomings mentioned above. In this paper, we introduce {CNN}-{LSTM} (Convolutional Neural Network, Long Short-Term Memory) deep learning based approach that can serve as a backbone for computer vision-based vibration measurement techniques. The key idea is to use each pixel of an image taken from an off the shelf camera, encapsulating the Spatio-temporal information, like a sensor to capture the modal frequencies of a vibrating structure. Non-contact “pixel-sensor” does not alter the system’s dynamics and is relatively low-cost, agile, and provides measurements with very high spatial resolution. Our computer vision-based deep learning model takes the video of a vibrating structure as input and outputs the fundamental modal frequencies. We demonstrate, using reliable empirical results, that “pixel-sensor” is more efficient, autonomous, and accurate. Robustness of the deep learning model has been put to the test by using specimens of a variety of materials, and varying dimensions and results have shown high levels of sensing accuracy.},
	pages = {106885},
	journaltitle = {Mechanical Systems and Signal Processing},
	shortjournal = {Mechanical Systems and Signal Processing},
	author = {Yang, Ruoyu and Singh, Shubhendu Kumar and Tavakkoli, Mostafa and Amiri, Nikta and Yang, Yongchao and Karami, M. Amin and Rai, Rahul},
	urldate = {2024-08-30},
	date = {2020-10-01},
	keywords = {{CNN} (convolutional neural network), Computer vision, {LSTM} (long short-term memory), Modal analysis},
	file = {ScienceDirect Snapshot:/Users/jjmr/Zotero/storage/EZXF62LV/S0888327020302715.html:text/html},
}

@article{kong_automatic_2018,
	title = {Automatic analysis of complex athlete techniques in broadcast taekwondo video},
	volume = {77},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-017-4979-0},
	doi = {10.1007/s11042-017-4979-0},
	pages = {13643--13660},
	number = {11},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Kong, Yongqiang and Wei, Zhengang and Huang, Shanshan},
	urldate = {2024-08-30},
	date = {2018-06},
	langid = {english},
	file = {PDF:/Users/jjmr/Zotero/storage/VICHNRAW/Kong et al. - 2018 - Automatic analysis of complex athlete techniques in broadcast taekwondo video.pdf:application/pdf},
}

@article{lee_tuhad_2020,
	title = {{TUHAD}: Taekwondo Unit Technique Human Action Dataset with Key Frame-Based {CNN} Action Recognition},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/17/4871},
	doi = {10.3390/s20174871},
	shorttitle = {{TUHAD}},
	abstract = {In taekwondo, poomsae (i.e., form) competitions have no quantitative scoring standards, unlike gyeorugi (i.e., full-contact sparring) in the Olympics. Consequently, there are diverse fairness issues regarding poomsae evaluation, and the demand for quantitative evaluation tools is increasing. Action recognition is a promising approach, but the extreme and rapid actions of taekwondo complicate its application. This study established the Taekwondo Unit technique Human Action Dataset ({TUHAD}), which consists of multimodal image sequences of poomsae actions. {TUHAD} contains 1936 action samples of eight unit techniques performed by 10 experts and captured by two camera views. A key frame-based convolutional neural network architecture was developed for taekwondo action recognition, and its accuracy was validated for various input configurations. A correlation analysis of the input configuration and accuracy demonstrated that the proposed model achieved a recognition accuracy of up to 95.833\% (lowest accuracy of 74.49\%). This study contributes to the research and development of taekwondo action recognition.},
	pages = {4871},
	number = {17},
	journaltitle = {Sensors},
	author = {Lee, Jinkue and Jung, Hoeryong},
	urldate = {2024-08-30},
	date = {2020-01},
	langid = {english},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {action recognition, human action dataset, taekwondo, convolutional neural network, gesture recognition, poomsae},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/E23SM2D4/Lee y Jung - 2020 - TUHAD Taekwondo Unit Technique Human Action Dataset with Key Frame-Based CNN Action Recognition.pdf:application/pdf},
}

@article{xie_realization_2022,
	title = {Realization of Intelligent Scoring System of Taekwondo Protective Gear under the Application of Neural Network {BP} Model},
	volume = {2022},
	issn = {1687-5265},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9259272/},
	doi = {10.1155/2022/5902983},
	abstract = {This article presents the novel design of the technology acceptance model to create a predictive model of whether or not people will accept Taekwondo safety gear. A survey containing 28 items was completed by 200 collegiate Taekwondo practitioners associated with the Taekwondo Association. A significance level of 0.05 was employed for the correlation and structural equation modeling analyses. The perceived usefulness of the proposed system is practical compared to the existing system, which is significantly influenced by perceived quality. Perceived ease of use and perceived usefulness were also unaffected by visual beauty. Wearability had a substantial impact on perceived ease of use but significantly negatively impacted perceived usefulness. Perceived ease of use and perceived usefulness were not affected by the functionality of the proposed system. Perceived usefulness is significantly influenced by perceived ease of use, and the acceptance intention is affected by perceived usefulness which also affects the system's performance. These findings imply that increasing the device's perceived quality and wearability will increase its acceptance. This research shows an adequate verification model to validate the desired range of signals to accept Taekwondo electronic protective devices.},
	pages = {5902983},
	journaltitle = {Computational Intelligence and Neuroscience},
	shortjournal = {Comput Intell Neurosci},
	author = {Xie, Xiaoqing and Tan, Fei},
	urldate = {2024-08-30},
	date = {2022-06-29},
	pmid = {35814568},
	pmcid = {PMC9259272},
	file = {PubMed Central Full Text PDF:/Users/jjmr/Zotero/storage/RHFKZUFD/Xie y Tan - 2022 - Realization of Intelligent Scoring System of Taekwondo Protective Gear under the Application of Neur.pdf:application/pdf},
}

@article{lim_action_2024,
	title = {Action Recognition of Taekwondo Unit Actions Using Action Images Constructed with Time-Warped Motion Profiles},
	volume = {24},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/24/8/2595},
	doi = {10.3390/s24082595},
	abstract = {Taekwondo has evolved from a traditional martial art into an official Olympic sport. This study introduces a novel action recognition model tailored for Taekwondo unit actions, utilizing joint-motion data acquired via wearable inertial measurement unit ({IMU}) sensors. The utilization of {IMU} sensor-measured motion data facilitates the capture of the intricate and rapid movements characteristic of Taekwondo techniques. The model, underpinned by a conventional convolutional neural network ({CNN})-based image classification framework, synthesizes action images to represent individual Taekwondo unit actions. These action images are generated by mapping joint-motion profiles onto the {RGB} color space, thus encapsulating the motion dynamics of a single unit action within a solitary image. To further refine the representation of rapid movements within these images, a time-warping technique was applied, adjusting motion profiles in relation to the velocity of the action. The effectiveness of the proposed model was assessed using a dataset compiled from 40 Taekwondo experts, yielding remarkable outcomes: an accuracy of 0.998, a precision of 0.983, a recall of 0.982, and an F1 score of 0.982. These results underscore this time-warping technique’s contribution to enhancing feature representation, as well as the proposed method’s scalability and effectiveness in recognizing Taekwondo unit actions.},
	pages = {2595},
	number = {8},
	journaltitle = {Sensors},
	author = {Lim, Junghwan and Luo, Chenglong and Lee, Seunghun and Song, Young Eun and Jung, Hoeryong},
	urldate = {2024-08-30},
	date = {2024-01},
	langid = {english},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {action recognition, convolution neural network, human action dataset, taekwondo},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/2LD37YRH/Lim et al. - 2024 - Action Recognition of Taekwondo Unit Actions Using Action Images Constructed with Time-Warped Motion.pdf:application/pdf},
}
