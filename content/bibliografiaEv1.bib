
@article{hachaj_application_2015,
	title = {Application of {Assistive} {Computer} {Vision} {Methods} to {Oyama} {Karate} {Techniques} {Recognition}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/7/4/1670},
	doi = {10.3390/sym7041670},
	abstract = {In this paper we propose a novel algorithm that enables online actions segmentation and classification. The algorithm enables segmentation from an incoming motion capture (MoCap) data stream, sport (or karate) movement sequences that are later processed by classification algorithm. The segmentation is based on Gesture Description Language classifier that is trained with an unsupervised learning algorithm. The classification is performed by continuous density forward-only hidden Markov models (HMM) classifier. Our methodology was evaluated on a unique dataset consisting of MoCap recordings of six Oyama karate martial artists including multiple champion of Kumite Knockdown Oyama karate. The dataset consists of 10 classes of actions and included dynamic actions of stands, kicks and blocking techniques. Total number of samples was 1236. We have examined several HMM classifiers with various number of hidden states and also Gaussian mixture model (GMM) classifier to empirically find the best setup of the proposed method in our dataset. We have used leave-one-out cross validation. The recognition rate of our methodology differs between karate techniques and is in the range of 81\% ± 15\% even to 100\%. Our method is not limited for this class of actions but can be easily adapted to any other MoCap-based actions. The description of our approach and its evaluation are the main contributions of this paper. The results presented in this paper are effects of pioneering research on online karate action classification.},
	language = {en},
	number = {4},
	urldate = {2024-08-20},
	journal = {Symmetry},
	author = {Hachaj, Tomasz and Ogiela, Marek R. and Koptyra, Katarzyna},
	month = dec,
	year = {2015},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {actions segmentation, assistive computer vision, Gestures Description Language, hidden Markov models, Oyama karate, sport actions recognition, unsupervised learning},
	pages = {1670--1698},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/WN7HVJAK/Hachaj et al. - 2015 - Application of Assistive Computer Vision Methods to Oyama Karate Techniques Recognition.pdf:application/pdf},
}

@article{echeverria_kumitron_2021,
	title = {{KUMITRON}: {Artificial} {Intelligence} {System} to {Monitor} {Karate} {Fights} that {Synchronize} {Aerial} {Images} with {Physiological} and {Inertial} {Signals}},
	shorttitle = {{KUMITRON}},
	url = {https://dl.acm.org/doi/10.1145/3397482.3450730},
	doi = {10.1145/3397482.3450730},
	abstract = {New technologies make it possible to develop tools that allow more efficient and personalized interaction in unsuspected areas such as martial arts. From the point of view of the modelling of human movement in relation to the learning of complex motor skills, martial arts are of interest because they are articulated around a system of movements that are predefined -or at least, bounded- and governed by the Laws of Physics. Their execution must be learned after continuous practice over time. Artificial Intelligence algorithms can be used to obtain motion patterns that can be used to compare a learners’ practice against the execution of an expert, as well as to analyse its temporal evolution during learning. In this paper we introduce KUMITRON, which collects motion data from wearable sensors and integrates computer vision and machine learning algorithms to help karate practitioners improve their skills in combat. The current version focuses on using the computer vision algorithms to identify the anticipation of the opponent's movements. This information is computed in real time and can be communicated to the learner together with a recommendation of the type of strategy to use in the combat.},
	language = {en},
	urldate = {2024-08-20},
	journal = {26th International Conference on Intelligent User Interfaces},
	author = {Echeverria, Jon and C. Santos, Olga},
	month = apr,
	year = {2021},
	note = {Conference Name: IUI '21: 26th International Conference on Intelligent User Interfaces
ISBN: 9781450380188
Place: College Station TX USA
Publisher: ACM},
	pages = {37--39},
}

@article{al-faris_review_2020,
	title = {A {Review} on {Computer} {Vision}-{Based} {Methods} for {Human} {Action} {Recognition}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/6/6/46},
	doi = {10.3390/jimaging6060046},
	abstract = {Human action recognition targets recognising different actions from a sequence of observations and different environmental conditions. A wide different applications is applicable to vision based action recognition research. This can include video surveillance, tracking, health care, and human–computer interaction. However, accurate and effective vision based recognition systems continue to be a big challenging area of research in the field of computer vision. This review introduces the most recent human action recognition systems and provides the advances of state-of-the-art methods. To this end, the direction of this research is sorted out from hand-crafted representation based methods including holistic and local representation methods with various sources of data, to a deep learning technology including discriminative and generative models and multi-modality based methods. Next, the most common datasets of human action recognition are presented. This review introduces several analyses, comparisons and recommendations that help to find out the direction of future research.},
	language = {en},
	number = {6},
	urldate = {2024-08-29},
	journal = {Journal of Imaging},
	author = {Al-Faris, Mahmoud and Chiverton, John and Ndzi, David and Ahmed, Ahmed Isam},
	month = jun,
	year = {2020},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, feature representation, hand-crafted feature, human action recognition},
	pages = {46},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/7HKMVT7D/Al-Faris et al. - 2020 - A Review on Computer Vision-Based Methods for Human Action Recognition.pdf:application/pdf},
}

@article{azmat_aerial_2023,
	title = {Aerial {Insights}: {Deep} {Learning}-{Based} {Human} {Action} {Recognition} in {Drone} {Imagery}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {Aerial {Insights}},
	url = {https://ieeexplore.ieee.org/document/10210047},
	doi = {10.1109/ACCESS.2023.3302353},
	abstract = {Human action recognition is critical because it allows machines to comprehend and interpret human behavior, which has several real-world applications such as video surveillance, robot-human collaboration, sports analysis, and entertainment. The enormous variety in human motion and appearance is one of the most challenging problems in human action recognition. Additionally, when drones are employed for video capture, the complexity of recognition gets enhanced manyfold. The challenges including the dynamic background, motion blur, occlusions, video capture angle, and exposure issues gets introduced that need to be taken care of. In this article, we proposed a system that deal with the mentioned challenges in drone recorded red-green-blue (RGB) videos. The system first splits the video into its constituent frames and then performs a focused smoothing operation on the frames utilizing a bilateral filter. As a result, the foreground objects in the image gets enhanced while the background gets blur. After that, a segmentation operation is performed using a quick shift segmentation algorithm that separates out human silhouette from the original video frame. The human skeleton was extracted from the silhouette, and key-points on the skeleton were identified. Thirteen skeleton key-points were extracted, including the head, left wrist, right wrist, left elbow, right elbow, torso, abdomen, right thigh, left thigh, right knee, left knee, right ankle, and left ankle. Using these key-points, we extracted normalized positions, their angular and distance relationship with each other, and 3D point clouds. By implementing an expectation maximization algorithm based on the Gaussian mixture model, we drew elliptical clusters over the pixels using the key-points as the central positions to represent the human silhouette. Landmarks were located on the boundaries of these ellipses and were tracked from the beginning until the end of activity. After optimizing the feature matrix using a naïve Bayes feature optimizer, the classification is performed using a deep convolutional neural network. For our experimentation and the validation of our system, three benchmark datasets were utilized i.e., the UAVGesture, the DroneAction, and the UAVHuman dataset. Our model achieved a respective action recognition accuracy of 0.95, 0.90, and 0.44 on the mentioned datasets.},
	urldate = {2024-08-30},
	journal = {IEEE Access},
	author = {Azmat, Usman and Alotaibi, Saud S. and Abdelhaq, Maha and Alsufyani, Nawal and Shorfuzzaman, Mohammad and Jalal, Ahmad and Park, Jeongmin},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Behavioral sciences, Cameras, Convolutional neural network, Convolutional neural networks, Drones, expectation maximization, Feature extraction, Gaussian mixture model, Human activity recognition, Image segmentation, quadratic discriminant analysis, Quadrature phase shift keying, quick-shift segmentation, Three-dimensional displays, video processing},
	pages = {83946--83961},
	file = {IEEE Xplore Abstract Record:/Users/jjmr/Zotero/storage/MXV8VULD/10210047.html:text/html;IEEE Xplore Full Text PDF:/Users/jjmr/Zotero/storage/C73E7L3S/Azmat et al. - 2023 - Aerial Insights Deep Learning-Based Human Action Recognition in Drone Imagery.pdf:application/pdf},
}

@article{bhatt_cnn_2021,
	title = {{CNN} {Variants} for {Computer} {Vision}: {History}, {Architecture}, {Application}, {Challenges} and {Future} {Scope}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {{CNN} {Variants} for {Computer} {Vision}},
	url = {https://www.mdpi.com/2079-9292/10/20/2470},
	doi = {10.3390/electronics10202470},
	abstract = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN’s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.},
	language = {en},
	number = {20},
	urldate = {2024-08-30},
	journal = {Electronics},
	author = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
	month = jan,
	year = {2021},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {attention-based CNN, CNN, computer vision, deep CNN, feature-map exploitation, object recognition},
	pages = {2470},
	file = {Full Text PDF:/Users/jjmr/Zotero/storage/JUIEEKNF/Bhatt et al. - 2021 - CNN Variants for Computer Vision History, Architecture, Application, Challenges and Future Scope.pdf:application/pdf},
}

@article{yang_cnn-lstm_2020,
	title = {{CNN}-{LSTM} deep learning architecture for computer vision-based modal frequency detection},
	volume = {144},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327020302715},
	doi = {10.1016/j.ymssp.2020.106885},
	abstract = {The conventional modal analysis involves physically-attached wired or wireless sensors for vibration measurement of structures. However, this method has certain disadvantages, owing to the sensor’s weight and its low spatial resolution, which limits the analysis precision or the high cost of optical vibration sensors. Besides, the sensor installation and calibration in itself is a time consuming and labor-intensive process. Non-contact computer vision-based vibration measurement techniques can address the shortcomings mentioned above. In this paper, we introduce CNN-LSTM (Convolutional Neural Network, Long Short-Term Memory) deep learning based approach that can serve as a backbone for computer vision-based vibration measurement techniques. The key idea is to use each pixel of an image taken from an off the shelf camera, encapsulating the Spatio-temporal information, like a sensor to capture the modal frequencies of a vibrating structure. Non-contact “pixel-sensor” does not alter the system’s dynamics and is relatively low-cost, agile, and provides measurements with very high spatial resolution. Our computer vision-based deep learning model takes the video of a vibrating structure as input and outputs the fundamental modal frequencies. We demonstrate, using reliable empirical results, that “pixel-sensor” is more efficient, autonomous, and accurate. Robustness of the deep learning model has been put to the test by using specimens of a variety of materials, and varying dimensions and results have shown high levels of sensing accuracy.},
	urldate = {2024-08-30},
	journal = {Mechanical Systems and Signal Processing},
	author = {Yang, Ruoyu and Singh, Shubhendu Kumar and Tavakkoli, Mostafa and Amiri, Nikta and Yang, Yongchao and Karami, M. Amin and Rai, Rahul},
	month = oct,
	year = {2020},
	keywords = {CNN (convolutional neural network), Computer vision, LSTM (long short-term memory), Modal analysis},
	pages = {106885},
	file = {ScienceDirect Snapshot:/Users/jjmr/Zotero/storage/EZXF62LV/S0888327020302715.html:text/html},
}
